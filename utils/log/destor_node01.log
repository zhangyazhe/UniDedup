destor_server_process
destor_server_process: receive a request!
type: 0 
Init recipe store successfully
Init container store successfully
Init index module successfully
==== backup begin ====
Read 0 inherited sparse containers
append thread start
pkt size: 5
write container buffer1
push container buffer
[debug] destor in append thread, get one chunk from buffer.
write_container start
append thread end
append thread start
write container buffer2
push container buffer
[debug] destor in append thread, get one chunk from buffer.
write_container start
append thread end
append thread start
read phase stops successfully!
chunk phase stops successfully!
hash phase stops successfully!
dedup phase stops successfully: 1 segments of 1024 chunks on average
rewrite phase stops successfully!
Record 0 sparse containers, and 0 of them are inherited
CMA: update 2 records.
filter phase stops successfully!
flushing hash table!
flushing hash table successfully!
[debug] destor in append thread, break.
append phase stops successfully!
==== backup end ====
job id: 0
backup path: /hadoop42
number of files: 1
number of chunks: 1024 (4797 bytes on average)
number of unique chunks: 1024
total size(B): 4912813
stored data size(B): 4912813
deduplication ratio: 0.0000, 1.0000
total time(s): 0.000
throughput(MB/s): inf
number of zero chunks: 0
size of zero chunks: 0
number of rewritten chunks: 0
size of rewritten chunks: 0
rewritten rate in size: 0.000
read_time : 0.093s, 50.53MB/s
chunk_time : 0.015s, 315.50MB/s
hash_time : 0.019s, 250.81MB/s
dedup_time : 0.001s, 5081.59MB/s
rewrite_time : 0.000s, 126627.66MB/s
filter_time : 0.005s, 956.17MB/s
write_time : 0.000s, 17225.09MB/s
1
2
destor_server_process
destor_server_process: receive a request!
type: 0 
Init recipe store successfully
[debug] destor init_container_store, in read mode.
Init container store successfully
Init index module successfully
==== backup begin ====
pkt size: 5
append thread start
Read 0 inherited sparse containers
write container buffer1
push container buffer
[debug] destor in append thread, get one chunk from buffer.
write_container start
append thread end
append thread start
write container buffer2
push container buffer
[debug] destor in append thread, get one chunk from buffer.
write_container start
append thread end
append thread start
read phase stops successfully!
chunk phase stops successfully!
hash phase stops successfully!
dedup phase stops successfully: 1 segments of 684 chunks on average
rewrite phase stops successfully!
Record 1 sparse containers, and 0 of them are inherited
CMA: read 2 records.
CMA: update 4 records.
filter phase stops successfully!
flushing hash table!
flushing hash table successfully!
[debug] destor in append thread, break.
append phase stops successfully!
==== backup end ====
job id: 1
backup path: /hadoop1010
number of files: 1
number of chunks: 684 (6334 bytes on average)
number of unique chunks: 684
total size(B): 4333139
stored data size(B): 4333139
deduplication ratio: 0.0000, 1.0000
total time(s): 0.000
throughput(MB/s): inf
number of zero chunks: 0
size of zero chunks: 0
number of rewritten chunks: 0
size of rewritten chunks: 0
rewritten rate in size: 0.000
read_time : 0.085s, 48.69MB/s
chunk_time : 0.014s, 292.04MB/s
hash_time : 0.015s, 273.45MB/s
dedup_time : 0.000s, 10488.33MB/s
rewrite_time : 0.000s, 93918.26MB/s
filter_time : 0.008s, 541.95MB/s
write_time : 0.000s, 34436.69MB/s
1
2
